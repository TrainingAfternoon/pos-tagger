{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Hidden Markov: POS Tagging\n",
    "Sam Keyser, Carter Shavitz, John Paul Bunn\n",
    "\n",
    "CS 2400 - Introduction to AI\n",
    "\n",
    "## Experiment\n",
    "### Set Up"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\keysers\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\keysers\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     C:\\Users\\keysers\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\keysers\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import brown, treebank, conll2000\n",
    "\n",
    "# Download the requisite datasets\n",
    "nltk.download('treebank')\n",
    "nltk.download('brown')\n",
    "nltk.download('conll2000')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "# Load datasets\n",
    "treebank_corpus = treebank.tagged_sents(tagset='universal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Pierre', 'NOUN'), ('Vinken', 'NOUN'), (',', '.'), ('61', 'NUM'), ('years', 'NOUN'), ('old', 'ADJ'), (',', '.'), ('will', 'VERB'), ('join', 'VERB'), ('the', 'DET'), ('board', 'NOUN'), ('as', 'ADP'), ('a', 'DET'), ('nonexecutive', 'ADJ'), ('director', 'NOUN'), ('Nov.', 'NOUN'), ('29', 'NUM'), ('.', '.')], [('Mr.', 'NOUN'), ('Vinken', 'NOUN'), ('is', 'VERB'), ('chairman', 'NOUN'), ('of', 'ADP'), ('Elsevier', 'NOUN'), ('N.V.', 'NOUN'), (',', '.'), ('the', 'DET'), ('Dutch', 'NOUN'), ('publishing', 'VERB'), ('group', 'NOUN'), ('.', '.')], ...]\n",
      "Sentence: ['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.']\n",
      "Tags: ['NOUN', 'NOUN', '.', 'NUM', 'NOUN', 'ADJ', '.', 'VERB', 'VERB', 'DET', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'NOUN', 'NUM', '.']\n"
     ]
    }
   ],
   "source": [
    "print(treebank_corpus)\n",
    "\n",
    "# Get a test X, y out of the corpus\n",
    "X, y = zip(*treebank_corpus[0])\n",
    "X = list(X)\n",
    "y = list(y)\n",
    "print('Sentence:', X)\n",
    "print('Tags:', y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Probability Counting\n",
    "Now that we've got a set of test sentences and tags, we need to start constructing the transition and emission probabilities. This count should be a function *N*, which is the length of the *N*-gram which we use to keep track of previous states up to the current one."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Playing around with Splitting Sentences into *N*-grams"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "N = 2 # Default N-gram length"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Example of splitting using ngram from nltk"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.']\n",
      "('Pierre', 'Vinken') ('Vinken', ',') (',', '61') ('61', 'years') ('years', 'old') ('old', ',') (',', 'will') ('will', 'join') ('join', 'the') ('the', 'board') ('board', 'as') ('as', 'a') ('a', 'nonexecutive') ('nonexecutive', 'director') ('director', 'Nov.') ('Nov.', '29') ('29', '.')\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print(*ngrams(X, N)) # Split up our X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "### Playing around with Conditional Probability"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Counting probability based on the article [here](https://www.freecodecamp.org/news/a-deep-dive-into-part-of-speech-tagging-using-viterbi-algorithm-17c8de32e8bc)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states: {'Awake', '.', 'Asleep'}\n",
      "obs: ['*', 'Noise', 'Quiet', 'Quiet', 'Noise', '*']\n",
      "tags: ['.', 'Awake', 'Awake', 'Asleep', 'Awake', '.']\n",
      "\n",
      "counts\n",
      "{'.': 2, ('*', '.'): 2, 'Awake': 3, ('Noise', 'Awake'): 2, ('Quiet', 'Awake'): 1, 'Asleep': 1, ('Quiet', 'Asleep'): 1}\n",
      "\n",
      "emission probabilities\n",
      "{('*', '.'): 1.0, ('Noise', 'Awake'): 0.6666666666666666, ('Quiet', 'Awake'): 0.3333333333333333, ('Quiet', 'Asleep'): 1.0}\n",
      "\n",
      "('.', 'Awake')\n",
      "('Awake', 'Awake')\n",
      "('Awake', 'Asleep')\n",
      "('Asleep', 'Awake')\n",
      "('Awake', '.')\n",
      "\n",
      "transition probabilities\n",
      "{'Awake': 3, ('.', 'Awake'): 1.0, ('.',): 1, ('Awake', 'Awake'): 0.3333333333333333, ('Awake',): 3, 'Asleep': 1, ('Awake', 'Asleep'): 0.3333333333333333, ('Asleep', 'Awake'): 1.0, ('Asleep',): 1, '.': 1, ('Awake', '.'): 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "N = 2\n",
    "X = ['Noise', 'Quiet', 'Quiet', 'Noise']\n",
    "y = ['Awake', 'Awake', 'Asleep', 'Awake']\n",
    "\n",
    "# pre/append start/end\n",
    "X.insert(0, '*')\n",
    "X.append('*')\n",
    "\n",
    "y.insert(0, '.')\n",
    "y.append('.')\n",
    "\n",
    "\n",
    "states = set(y) # all the states we can see\n",
    "print('states:', states)\n",
    "\n",
    "obs = X\n",
    "print('obs:', X)\n",
    "print('tags:', y)\n",
    "\n",
    "c = {} # counts\n",
    "for observation, state in zip(X, y):\n",
    "    c[state] = c.get(state, 0) + 1\n",
    "    c[(observation, state)] = c.get((observation, state), 0) + 1\n",
    "\n",
    "e = {} # emission probabilities\n",
    "for observation, state in zip(X, y):\n",
    "    e[(observation, state)] = c[(observation, state)] / c[state]\n",
    "\n",
    "print('\\ncounts')\n",
    "print(c)\n",
    "print('\\nemission probabilities')\n",
    "print(e)\n",
    "\n",
    "state_c = {}\n",
    "gram_c = {} # gram counts\n",
    "q = {} # transition probabilities\n",
    "# for gram in ngrams(y, N):\n",
    "#     gram_c[gram] = gram_c.get(gram, 0) + 1\n",
    "#\n",
    "# for state in y:\n",
    "#     state_c[state] = state_c.get(state, 0) + 1\n",
    "\n",
    "print()\n",
    "for gram in ngrams(y, N):\n",
    "    print(gram)\n",
    "    q[gram[-1]] = q.get(gram[-1], 0) + 1\n",
    "    q[gram] = q.get(gram, 0) + 1\n",
    "    q[gram[:-1]] = q.get(gram[:-1], 0) + 1\n",
    "\n",
    "for gram in q.keys():\n",
    "    if len(gram) == N:\n",
    "        q[gram] = q[gram]/q[gram[:-1]]\n",
    "\n",
    "\n",
    "# print('\\nngram counts')\n",
    "# print(gram_c)\n",
    "# print('\\nstate counts')\n",
    "# print(state_c)\n",
    "print('\\ntransition probabilities')\n",
    "print(q)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### More Rigorous Conditional Probability\n",
    "First, lets reload our test data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "[[('Pierre', 'NOUN'), ('Vinken', 'NOUN'), (',', '.'), ('61', 'NUM'), ('years', 'NOUN'), ('old', 'ADJ'), (',', '.'), ('will', 'VERB'), ('join', 'VERB'), ('the', 'DET'), ('board', 'NOUN'), ('as', 'ADP'), ('a', 'DET'), ('nonexecutive', 'ADJ'), ('director', 'NOUN'), ('Nov.', 'NOUN'), ('29', 'NUM'), ('.', '.')], [('Mr.', 'NOUN'), ('Vinken', 'NOUN'), ('is', 'VERB'), ('chairman', 'NOUN'), ('of', 'ADP'), ('Elsevier', 'NOUN'), ('N.V.', 'NOUN'), (',', '.'), ('the', 'DET'), ('Dutch', 'NOUN'), ('publishing', 'VERB'), ('group', 'NOUN'), ('.', '.')], ...]"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 3\n",
    "#X = [[word for word, tag in sentence] for sentence in treebank_corpus]\n",
    "#y = [[tag for word, tag in sentence] for sentence in treebank_corpus]\n",
    "X = treebank_corpus\n",
    "X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We write the following conditional probability class based on [this](https://github.com/edorado93/HMM-Part-of-Speech-Tagger/blob/master/hmmlearn.py)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing conditional_prob.py\n"
     ]
    }
   ],
   "source": [
    "%%file conditional_prob.py\n",
    "import math\n",
    "import itertools\n",
    "import functools\n",
    "\n",
    "# convenience method\n",
    "def increment_dict_val(dict, val):\n",
    "    dict[val] = dict.get(val, 0) + 1\n",
    "\n",
    "class ConditionalProbability:\n",
    "    start_tag = '!@#$START$#@!' # Pick some arbitrary string that is not likely to be naturally embedded into our text data\n",
    "\n",
    "    k = 10 # k fold hyperparameter\n",
    "\n",
    "    def __init__(self, X, N=3):\n",
    "        assert(N >= 2)\n",
    "\n",
    "        # Init dataset and hyperparameters\n",
    "        self.X = X\n",
    "        #self.y = y\n",
    "        self.N = N\n",
    "\n",
    "        # P(Wi|Ck)\n",
    "        self.words_given_pos = {}\n",
    "\n",
    "        # conditional probability of P(Xi+N|Xi+N-1...Xi)\n",
    "        self.full_ngram = {}\n",
    "\n",
    "        # maps word to set of tags associated with it\n",
    "        self.word_to_tag = {}\n",
    "\n",
    "        # maps (word, tag) pairings to # of appearances\n",
    "        self.word_tag_count = {}\n",
    "\n",
    "        # tracks the number of time a tag has appeared\n",
    "        self.tag_count = {{}}\n",
    "\n",
    "        # counts the occurrence of n-gram of tags\n",
    "        self.ngram_counts = {}\n",
    "\n",
    "        # counts the occurrences of (n-1)-grams of tags\n",
    "        self.subset_ngram_counts = {}\n",
    "\n",
    "        # set of all tags we've seen\n",
    "        self.tags = set()\n",
    "\n",
    "        # set of all words\n",
    "        self.words = set()\n",
    "\n",
    "        ''' BACK-OFF PROB '''\n",
    "        self.transition_backoff = {}\n",
    "        self.emission_backoff = {}\n",
    "\n",
    "        ''' SINGLETON COUNTS '''\n",
    "        self.transition_singleton = {}\n",
    "        self.emission_singleton = {}\n",
    "\n",
    "        ''' 1-COUNT SMOOTHED PROB '''\n",
    "        self.transition_one_count = {}\n",
    "        self.emission_smoothed = {}\n",
    "\n",
    "        self.n = 0 #TODO: what am I\n",
    "\n",
    "    def calculate_probabilities(self):\n",
    "        self.populate_dictionaries()\n",
    "        self.CFD_word_given_tag()\n",
    "        self.CFD_ngram_tags()\n",
    "        self.backoff_probabilities()\n",
    "        self.singleton_counts()\n",
    "        self.smooth_probabilities()\n",
    "        #self._save()\n",
    "\n",
    "    def populate_dictionaries(self):\n",
    "        self.pos_tags = set()\n",
    "\n",
    "        start_tag = ConditionalProbability.start_tag\n",
    "        for sentence in X:\n",
    "            for iter in range(0, self.N-1): # We need N-1 start characters to support N-grams\n",
    "                sentence.insert(0, (start_tag, start_tag))\n",
    "\n",
    "            start_idx = self.N-1\n",
    "            for idx in range(start_idx, len(sentence)):\n",
    "                ngram = tuple([sentence[jdx][1] for jdx in range(idx-N-1, idx+1)])\n",
    "                sub_ngram = ngram[:-1]\n",
    "\n",
    "                self.ngram_counts[ngram] = self.ngram_counts.get(ngram, 0) + 1\n",
    "                self.subset_ngram_counts[sub_ngram] = self.subset_ngram_counts.get(sub_ngram, 0) + 1\n",
    "\n",
    "            for word, tag in sentence:\n",
    "                self.n += 1\n",
    "\n",
    "                # do back off counts\n",
    "                self.transition_backoff[tag] = self.transition_backoff.get(tag, 0) + 1\n",
    "                self.emission_backoff[word] = self.emission_backoff.get(word, 0) + 1\n",
    "\n",
    "                self.tags.add(tag)\n",
    "                self.words.add(word)\n",
    "\n",
    "                increment_dict_val(self.word_tag_count, (word, tag))\n",
    "                increment_dict_val(self.tag_count, tag)\n",
    "\n",
    "                if word not in self.word_to_tag:\n",
    "                    self.word_to_tag[word] = set()\n",
    "                self.word_to_tag[word].add(tag)\n",
    "\n",
    "    def backoff_probabilities(self):\n",
    "        V = len(self.tags)\n",
    "\n",
    "        for word in self.emission_backoff:\n",
    "            self.emission_backoff[word] = float(1 + self.emission_backoff[word]) / float(self.n + V)\n",
    "\n",
    "        for tag in self.transition_backoff:\n",
    "            self.transition_backoff[tag] = float(self.transition_backoff[tag]) / float(self.n)\n",
    "\n",
    "    def singleton_counts(self):\n",
    "        for permutation in itertools.permutations(self.tags):\n",
    "            if permutation in self.ngram_counts and self.ngram_counts[permutation] == 1:\n",
    "                increment_dict_val(self.transition_singleton, permutation[1:])\n",
    "\n",
    "\n",
    "        for word in self.words:\n",
    "            for tag in self.tags:\n",
    "                word_tag = (word, tag)\n",
    "                if word_tag in self.word_tag_count and self.word_tag_count[word_tag] == 1:\n",
    "                    increment_dict_val(self.emission_singleton, tag)\n",
    "\n",
    "    def smooth_probabilities(self):\n",
    "        start_idx = self.N - 1\n",
    "\n",
    "        for sentence in self.X:\n",
    "            for idx in range(start_idx, len(sentence)):\n",
    "                ngram = tuple([sentence[jdx][1] for jdx in range(idx-N-1, idx+1)])\n",
    "                sub_ngram = ngram[:-1]\n",
    "                lamda = 1 + self.transition_singleton.get(sub_ngram, 0)\n",
    "                self.transition_one_count[ngram] = math.log(float(self.ngram_counts[ngram] + lamda * self.transition_backoff[sentence[idx][1]]) / float(self.subset_ngram_counts[sub_ngram] + lamda))\n",
    "\n",
    "        for word, tag_set in self.word_to_tag.items():\n",
    "            for tag in tag_set:\n",
    "                lamda = 1 + self.emission_singleton.get(tag, 0)\n",
    "                self.emission_smoothed[(word, tag)] = math.log(float(self.word_tag_count[(word, tag)] + lamda * self.emission_backoff[word]) / float(self.tag_count[tag] + lamda))\n",
    "\n",
    "    def CFD_word_given_tag(self):\n",
    "        '''\n",
    "        P(word|tag)\n",
    "        :return:\n",
    "        '''\n",
    "        for word, tag_set in self.word_to_tag.items():\n",
    "            for tag in tag_set:\n",
    "                self.words_given_pos[(word, tag)] = math.log(float(self.word_tag_count[(word, tag)]) / float(self.tag_count[tag]))\n",
    "\n",
    "    def CFD_ngram_tags(self):\n",
    "        '''\n",
    "        probability of a tag s given last n-1 tags\n",
    "        :return:\n",
    "        '''\n",
    "        start_idx = self.N-1\n",
    "        V = len(self.tags)\n",
    "\n",
    "        for sentence in X:\n",
    "            for idx in range(start_idx, len(sentence)):\n",
    "                ngram = tuple([sentence[jdx][1] for jdx in range(idx-N-1, idx+1)])\n",
    "                sub_ngram = ngram[:-1]\n",
    "\n",
    "                self.full_ngram[ngram] = math.log(float(1 + self.ngram_counts[ngram]) / float(V + self.subset_ngram_counts[sub_ngram]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}