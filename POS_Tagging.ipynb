{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Hidden Markov: POS Tagging\n",
    "Sam Keyser, Carter Shavitz, John Paul Bunn\n",
    "\n",
    "CS 2400 - Introduction to AI\n",
    "\n",
    "## Experiment\n",
    "### Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\keysers\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\keysers\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     C:\\Users\\keysers\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\keysers\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import brown, treebank, conll2000\n",
    "\n",
    "# Download the requisite datasets\n",
    "nltk.download('treebank')\n",
    "nltk.download('brown')\n",
    "nltk.download('conll2000')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "# Load datasets\n",
    "treebank_corpus = treebank.tagged_sents(tagset='universal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Pierre', 'NOUN'), ('Vinken', 'NOUN'), (',', '.'), ('61', 'NUM'), ('years', 'NOUN'), ('old', 'ADJ'), (',', '.'), ('will', 'VERB'), ('join', 'VERB'), ('the', 'DET'), ('board', 'NOUN'), ('as', 'ADP'), ('a', 'DET'), ('nonexecutive', 'ADJ'), ('director', 'NOUN'), ('Nov.', 'NOUN'), ('29', 'NUM'), ('.', '.')], [('Mr.', 'NOUN'), ('Vinken', 'NOUN'), ('is', 'VERB'), ('chairman', 'NOUN'), ('of', 'ADP'), ('Elsevier', 'NOUN'), ('N.V.', 'NOUN'), (',', '.'), ('the', 'DET'), ('Dutch', 'NOUN'), ('publishing', 'VERB'), ('group', 'NOUN'), ('.', '.')], ...]\n",
      "Sentence: ['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.']\n",
      "Tags: ['NOUN', 'NOUN', '.', 'NUM', 'NOUN', 'ADJ', '.', 'VERB', 'VERB', 'DET', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'NOUN', 'NUM', '.']\n"
     ]
    }
   ],
   "source": [
    "print(treebank_corpus)\n",
    "\n",
    "# Get a test X, y out of the corpus\n",
    "X, y = zip(*treebank_corpus[0])\n",
    "X = list(X)\n",
    "y = list(y)\n",
    "print('Sentence:', X)\n",
    "print('Tags:', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Probability Counting\n",
    "Now that we've got a set of test sentences and tags, we need to start constructing the transition and emission probabilities. This count should be a function *N*, which is the length of the *N*-gram which we use to keep track of previous states up to the current one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Playing around with Splitting Sentences into *N*-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "N = 3 # Default N-gram length\n",
    "start_tag ='!@#$%^&*()_+START+_)(*&^%$#@!' # some string of characters very unlikely to occur in the wild which we can use as start tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Example of splitting using ngram from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.']\n",
      "('Pierre', 'Vinken', ',') ('Vinken', ',', '61') (',', '61', 'years') ('61', 'years', 'old') ('years', 'old', ',') ('old', ',', 'will') (',', 'will', 'join') ('will', 'join', 'the') ('join', 'the', 'board') ('the', 'board', 'as') ('board', 'as', 'a') ('as', 'a', 'nonexecutive') ('a', 'nonexecutive', 'director') ('nonexecutive', 'director', 'Nov.') ('director', 'Nov.', '29') ('Nov.', '29', '.')\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print(*ngrams(X, N)) # Split up our X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Counting probability based on the article [here](https://www.freecodecamp.org/news/a-deep-dive-into-part-of-speech-tagging-using-viterbi-algorithm-17c8de32e8bc).\n",
    "\n",
    "\n",
    "We run over our dataset in X to fill out conditional probabilities for transition and emission, using naive laplace smoothing.\n",
    "\n",
    "The following section assumes we're using an *N* = 3 for simplicity.\n",
    "The actual implementation should be generic as to *N*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags: {'ADJ', 'VERB', 'NUM', 'DET', 'CONJ', 'X', 'ADP', 'PRON', 'PRT', 'NOUN', 'ADV', '.'}\n"
     ]
    }
   ],
   "source": [
    "X = treebank_corpus\n",
    "tmp = [[s for x, s in sentence] for sentence in X]\n",
    "tags = set([tag for tags in tmp for tag in tags])\n",
    "print('tags:', tags)\n",
    "\n",
    "# declare dictionaries\n",
    "C = {}\n",
    "q = {}\n",
    "e = {}\n",
    "\n",
    "lamda = 1 # laplace smoothing hyperparameter\n",
    "V = len(tags) # laplace smoothing hyperparameter\n",
    "\n",
    "# Convenience methods\n",
    "def increment_dict_val(dict, val):\n",
    "    dict[val] = dict.get(val, 0) + 1\n",
    "\n",
    "def safe_get(dict, val):\n",
    "    return dict.get(val, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.91 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Conditional prob counting\n",
    "for sentence in X:\n",
    "    # x = observation = word\n",
    "    # s = state = tag\n",
    "    words = [x for x, s in sentence]\n",
    "    tags = [s for x, s in sentence]\n",
    "\n",
    "    tags.insert(0, start_tag)\n",
    "    tags.insert(0, start_tag)\n",
    "\n",
    "    for trigram in ngrams(sentence, 3):\n",
    "        increment_dict_val(C, trigram)\n",
    "        increment_dict_val(C, trigram[:-1])\n",
    "\n",
    "    for x, s in sentence:\n",
    "        increment_dict_val(C, (x, s))\n",
    "\n",
    "    for trigram in ngrams(sentence, 3):\n",
    "        q[trigram[-1]] = (safe_get(q, trigram) + lamda) / (safe_get(q, trigram[:-1]) + lamda*V)\n",
    "\n",
    "    visited = set()\n",
    "    for x, s in sentence:\n",
    "        if s not in visited:\n",
    "            e[s] = (safe_get(e, (x,s)) + lamda) / (safe_get(e, (s)) + lamda*V)\n",
    "            visited.add(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pierre'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from viterbi import viterbi\n",
    "from viterbi import viterbi1\n",
    "\n",
    "sentence = X[0]\n",
    "\n",
    "C = {}\n",
    "q = {}\n",
    "e = {}\n",
    "words = [x for x, s in sentence]\n",
    "tags = [s for x, s in sentence]\n",
    "\n",
    "tags.insert(0, start_tag)\n",
    "tags.insert(0, start_tag)\n",
    "\n",
    "for trigram in ngrams(sentence, 3):\n",
    "    increment_dict_val(C, trigram)\n",
    "    increment_dict_val(C, trigram[:-1])\n",
    "\n",
    "for x, s in sentence:\n",
    "    increment_dict_val(C, (x, s))\n",
    "\n",
    "for trigram in ngrams(sentence, 3):\n",
    "    q[trigram[-1]] = (safe_get(q, trigram) + lamda) / (safe_get(q, trigram[:-1]) + lamda*V)\n",
    "\n",
    "visited = set()\n",
    "for x, s in sentence:\n",
    "    if s not in visited:\n",
    "        e[s] = (safe_get(e, (x,s)) + lamda) / (safe_get(e, (s)) + lamda*V)\n",
    "        visited.add(s)\n",
    "\n",
    "\n",
    "start_p = {k: 1.0/len(tags) for k in tags} # TODO: this would be a trainable parameter\n",
    "\n",
    "start_p[start_tag] = 1.0\n",
    "q[start_tag] = 1.0\n",
    "e[start_tag] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Word':words, \n",
    "        'Tag':tags}\n",
    "df = pd.DataFrame(data)\n",
    "viterbi1 = viterbi1(words, treebank_corpus, df, ) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
