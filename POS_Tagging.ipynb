{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Hidden Markov: POS Tagging\n",
    "Sam Keyser, Carter Shavitz, John Paul Bunn\n",
    "\n",
    "CS 2400 - Introduction to AI\n",
    "\n",
    "## Experiment\n",
    "### Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import brown, treebank, conll2000\n",
    "\n",
    "# Download the requisite datasets\n",
    "nltk.download('treebank')\n",
    "nltk.download('brown')\n",
    "nltk.download('conll2000')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "# Load datasets\n",
    "treebank_corpus = treebank.tagged_sents(tagset='universal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(treebank_corpus)\n",
    "\n",
    "# Get a test X, y out of the corpus\n",
    "X, y = zip(*treebank_corpus[0])\n",
    "X = list(X)\n",
    "y = list(y)\n",
    "print('Sentence:', X)\n",
    "print('Tags:', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Probability Counting\n",
    "Now that we've got a set of test sentences and tags, we need to start constructing the transition and emission probabilities. This count should be a function *N*, which is the length of the *N*-gram which we use to keep track of previous states up to the current one."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Playing around with Splitting Sentences into *N*-grams"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N = 3 # Default N-gram length\n",
    "start_tag ='!@#$%^&*()_+START+_)(*&^%$#@!' # some string of characters very unlikely to occur in the wild which we can use as start tag"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Example of splitting using ngram from nltk"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(X)\n",
    "print(*ngrams(X, N)) # Split up our X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Counting probability based on the article [here](https://www.freecodecamp.org/news/a-deep-dive-into-part-of-speech-tagging-using-viterbi-algorithm-17c8de32e8bc).\n",
    "\n",
    "\n",
    "We run over our dataset in X to fill out conditional probabilities for transition and emission, using naive laplace smoothing.\n",
    "\n",
    "The following section assumes we're using an *N* = 3 for simplicity.\n",
    "The actual implementation should be generic as to *N*."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tmp = [[s for x, s in sentence] for sentence in X]\n",
    "tags = set([tag for tags in tmp for tag in tags])\n",
    "print('tags:', tags)\n",
    "\n",
    "# declare dictionaries\n",
    "C = {}\n",
    "q = {}\n",
    "e = {}\n",
    "\n",
    "lamda = 1 # laplace smoothing hyperparameter\n",
    "V = len(tags) # laplace smoothing hyperparameter\n",
    "\n",
    "# Convenience methods\n",
    "def increment_dict_val(dict, val):\n",
    "    dict[val] = dict.get(val, 0) + 1\n",
    "\n",
    "def safe_get(dict, val):\n",
    "    return dict.get(val, 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "# Conditional prob counting\n",
    "for sentence in X:\n",
    "    # x = observation = word\n",
    "    # s = state = tag\n",
    "    words = [x for x, s in sentence]\n",
    "    tags = [s for x, s in sentence]\n",
    "\n",
    "    tags.insert(0, start_tag)\n",
    "    tags.insert(0, start_tag)\n",
    "\n",
    "    for trigram in ngrams(sentence, 3):\n",
    "        increment_dict_val(C, trigram)\n",
    "        increment_dict_val(C, trigram[:-1])\n",
    "\n",
    "    for x, s in sentence:\n",
    "        increment_dict_val(C, (x, s))\n",
    "\n",
    "    for trigram in ngrams(sentence, 3):\n",
    "        q[trigram[-1]] = (safe_get(q, trigram) + lamda) / (safe_get(q, trigram[:-1]) + lamda*V)\n",
    "\n",
    "    visited = set()\n",
    "    for x, s in sentence:\n",
    "        if s not in visited:\n",
    "            e[s] = (safe_get(e, (x,s)) + lamda) / (safe_get(e, (s)) + lamda*V)\n",
    "            visited.add(s)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}